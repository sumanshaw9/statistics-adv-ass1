{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgQIRiP14Vqv3fJAW+ypZB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumanshaw9/statistics-adv-ass1/blob/main/statistics_ass1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain the properties of the F-distribution.**"
      ],
      "metadata": {
        "id": "Ep8OKV0Q9Kav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is a continuous probability distribution that arises frequently in statistics, particularly in the analysis of variance (ANOVA) and hypothesis testing involving variances. Below are its key properties:\n",
        "\n",
        "1. Definition -\n",
        "- The F-distribution is the distribution of the ratio of two independent chi-squared distributed random variables divided by their respective degrees of freedom. It is commonly used to compare variances.\n",
        "\n",
        "2. Key Properties\n",
        "- a. Shape and Parameters - The shape of the F-distribution depends on two parameters:\n",
        "Degrees of Freedom (df₁): Numerator degrees of freedom.\n",
        "Degrees of Freedom (df₂): Denominator degrees of freedom.\n",
        "The distribution is positively skewed but becomes more symmetric as the degrees of freedom increase.\n",
        "\n",
        "- b. Range - The F-distribution is only defined for positive values.\n",
        "Its range is [0, ∞).\n",
        "\n",
        "- c. Mean - The mean of the F-distribution is defined as:\n",
        " - $\\text{Mean} = \\frac{\\text{df}_2}{\\text{df}_2 - 2}, $\n",
        " for ${\\text{df}_2} > 2$\n",
        "\n",
        " - Undefined if $df_2 \\leq 2. $\n",
        "-d. Variance\n",
        "  - The variance is:\n",
        "  $$Variance = \\frac{2 \\cdot df_2^2 \\cdot (df_1 + df_2 - 2)}{df_1 \\cdot (df_2 - 2)^2 \\cdot (df_2 - 4)}, \\quad \\text{for} \\quad df_2 > 4$$\n",
        "   - Undefined if $df_2 \\leq 4.$\n",
        "\n",
        " e. Mode\n",
        "The mode (most frequent value) is:\n",
        "$\\text{Mode} = \\frac{(\\text{df}_1 - 2)}{\\text{df}_1}{\\cdot}\\frac {\\text{df}_2}{(\\text{df}_2 + 2)}, $\n",
        "for ${\\text{df}_1} > 2$\n",
        "\n",
        "3. Characteristics\n",
        "- The F-distribution is not symmetric; it is positively skewed.\n",
        "As the numerator and denominator degrees of freedom increase, the F-distribution approaches a normal distribution.\n",
        "The total area under the F-distribution curve is equal to 1.\n",
        "4. Applications\n",
        "Hypothesis Testing: Used in ANOVA to test the equality of group variances.\n",
        "Model Comparison: Helps compare the fits of two nested regression models.\n",
        "Test for Equality of Variances: The F-test is used to compare the variances of two populations.\n",
        "\n",
        "5. Relationship with Other Distributions\n",
        "- The F-distribution is derived from the ratio of two chi-squared distributions.\n",
        "\n",
        "- $If ( X_1 \\sim \\chi^2(df_1) ) and ( X_2 \\sim \\chi^2(df_2) ),$ then:\n",
        "\n",
        "    $F = \\frac{\\left(X_1 / df_1\\right)}{\\left(X_2 / df_2\\right)} \\sim F(df_1, df_2)$\n",
        "\n",
        "- The square of a t-distributed variable with ${\\text{df}_1}$degrees of freedom is an F-distributed variable with parameters$(1,{\\text{df}_1)}$"
      ],
      "metadata": {
        "id": "rzQMhcuH9T9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?**"
      ],
      "metadata": {
        "id": "8os3j2K29IWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is used in statistical tests where variances are compared, making it a key tool in several important procedures. Here's an explanation of the types of tests and why the F-distribution is appropriate for them:\n",
        "\n",
        "1. ***Analysis of Variance (ANOVA)***\n",
        "- Purpose: To compare the means of three or more groups to determine if at least one group mean is significantly different from the others.\n",
        "- Why F-Distribution is Used:\n",
        " - ANOVA is based on the ratio of between-group variance to within-group variance.\n",
        " - The F-statistic follows an F-distribution under the null hypothesis because it is a ratio of two scaled chi-squared variables (variances).\n",
        " - Assumptions like normality and homogeneity of variances support the use of the F-distribution.\n",
        "2. ***Regression Analysis***\n",
        "- Purpose: To test the overall significance of a regression model or to compare multiple regression models.\n",
        "- Why F-Distribution is Used:\n",
        " - The F-test evaluates whether the variance explained by the model (due to the predictors) is significantly greater than the unexplained variance (residual error).\n",
        " - The test statistic for this is derived as a ratio of mean squares, which follows an F-distribution.\n",
        "3. ***Test of Equality of Variances***\n",
        "- Purpose: To test if two population variances are equal (e.g., in Levene's test or Bartlett's test).\n",
        "- Why F-Distribution is Used:\n",
        " - The test compares the ratio of the two sample variances. Under the null hypothesis (equal variances), this ratio follows an F-distribution.\n",
        "4. ***Comparing Nested Models***\n",
        "- Purpose: To compare two models where one is a special case of the other (nested models) to see if adding more parameters significantly improves the fit.\n",
        "- Why F-Distribution is Used:\n",
        " - The difference in residual sums of squares between the two models is divided by their respective degrees of freedom, forming an F-statistic.\n",
        " - This statistic follows an F-distribution under the null hypothesis.\n",
        "5. ***MANOVA (Multivariate Analysis of Variance)***\n",
        "- Purpose: To compare means of multiple dependent variables across groups.\n",
        "- Why F-Distribution is Used:\n",
        " - MANOVA extends ANOVA to multiple dependent variables, and the test statistic is based on ratios of variances, which follow an F-distribution.\n",
        "\n",
        "**Why the F-Distribution is Appropriate:**\n",
        "1. Ratio of Variances: The F-distribution arises naturally when comparing the ratio of two independent sample variances, which is the basis for many tests.\n",
        "2. Degrees of Freedom: The shape of the F-distribution is defined by the degrees of freedom of the numerator and denominator variances, making it flexible for various comparisons.\n",
        "3. Right-Skewed Nature: Since variances cannot be negative, the F-distribution is skewed to the right, reflecting the characteristics of variance ratios."
      ],
      "metadata": {
        "id": "OX8X4Kqf9QU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?**"
      ],
      "metadata": {
        "id": "y1uMS0MJ_LNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conduct an F-test for comparing the variances of two populations, several key assumptions must be met. These assumptions ensure that the results of the test are valid and reliable:\n",
        "\n",
        "1. **Both populations are normally distributed**\n",
        "The F-test assumes that the data in each population comes from a normal distribution.\n",
        "This is critical because the F-statistic is derived under the assumption of normality. Deviations from normality can lead to incorrect conclusions.\n",
        "How to check:\n",
        "Perform a normality test (e.g., Shapiro-Wilk test, Anderson-Darling test) or visualize the data using histograms or Q-Q plots.\n",
        "2.**The samples are independent**\n",
        "The samples taken from the two populations must be independent of each other.\n",
        "Independence ensures that the variances being compared are not influenced by any relationship or dependency between the samples.\n",
        "3.**The data is continuous**\n",
        "The F-test assumes that the data is continuous, meaning it is measured on an interval or ratio scale.\n",
        "Examples include measurements like height, weight, or temperature.\n",
        "4.**Variances being compared are from random samples**\n",
        "The data should be collected using a random sampling process to avoid bias.\n",
        "Random sampling ensures that the samples are representative of their respective populations.\n",
        "5. **The numerator and denominator degrees of freedom are fixed**\n",
        " - The degrees of freedom for the numerator and denominator, which correspond to the sample sizes of the two groups, should be properly accounted for.\n",
        "**Violations of Assumptions:**\n",
        " - Non-normality: If the normality assumption is violated, consider using a non-parametric test like the Levene’s test or Brown-Forsythe test, which are more robust to non-normality.\n",
        " - Dependent samples: If the samples are not independent, consider paired-sample techniques or adjust the analysis accordingly.\n",
        "\n",
        "\n",
        "**Summary Table of Assumptions:**\n",
        "\n",
        "| Assumption          | Purpose                                  | Check with                    |\n",
        "|---------------------|------------------------------------------|-------------------------------|\n",
        "| Normal distribution | Validates the derivation of F-statistic | Normality tests or visualization |\n",
        "| Independence of samples        | Ensures valid comparison between groups     | Study design verification      |\n",
        "| Continuous data      | Validates F-distribution application   | Data type assessment           |\n",
        "| Random sampling      | Avoids bias in variance comparison     | Sampling methodology check     |\n"
      ],
      "metadata": {
        "id": "U24E3gvi_RXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What is the purpose of ANOVA, and how does it differ from a t-test?**"
      ],
      "metadata": {
        "id": "ZgqmLkDoBlcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose of ANOVA**\n",
        "\n",
        "Analysis of Variance (ANOVA) is used to determine whether there are statistically significant differences between the means of three or more independent groups.\n",
        "\n",
        "- **When to Use**: ANOVA is particularly useful when:\n",
        "\n",
        "    1. There are more than two groups to compare.\n",
        "    2. You want to test for overall differences among groups rather than just between pairs of groups.\n",
        "- **Key Output**: The result of ANOVA is an F-statistic, which compares the variability between group means (between-group variance) to the variability within groups (within-group variance).\n",
        "\n",
        "**Purpose of a t-test**\n",
        "\n",
        "A **t-test** is used to compare the means of two groups to determine if they are significantly different from each other.\n",
        "\n",
        "- **When to Use**:\n",
        "\n",
        "  1. When comparing two groups only.\n",
        "  2. In small sample sizes where the underlying population is assumed to be normally distributed.\n",
        "\n",
        "**Key Types of t-tests:**\n",
        "- Independent t-test: Compares means of two independent groups.\n",
        "- Paired t-test: Compares means of two related groups (e.g., before and after measurements).\n",
        "\n",
        "**Key Differences Between ANOVA and t-test**\n",
        "\n",
        "| **Aspect**            | **ANOVA**                                                      | **t-test**                               |\n",
        "|------------------------|---------------------------------------------------------------|------------------------------------------|\n",
        "| **Number of Groups**   | Compares three or more groups.                                | Compares exactly two groups.             |\n",
        "| **Statistic Used**     | Produces an F-statistic.                                      | Produces a t-statistic.                  |\n",
        "| **Hypotheses Tested**  | Tests if all group means are equal.                           | Tests if the means of two groups are equal. |\n",
        "| **Post-hoc Analysis**  | Requires post-hoc tests (e.g., Tukey) to identify specific group differences if overall significance is found. | Does not require post-hoc tests.         |\n",
        "| **Complexity**         | Suitable for multi-group and multi-factor comparisons.        | Simpler, suitable for two-group comparisons. |\n",
        "| **Assumptions**        | Assumes normality, independence, and homogeneity of variance. | Same assumptions, but fewer groups simplify the test. |\n",
        "\n",
        "**Why Not Use Multiple t-tests Instead of ANOVA?**\n",
        "1. Increased Risk of Type I Error: Conducting multiple t-tests increases the probability of incorrectly rejecting the null hypothesis (false positives).\n",
        " - Example: For 3 groups, 3 t-tests would be needed, each with its own chance of error.\n",
        "2. ANOVA Controls Error Rate: ANOVA maintains the overall Type I error rate, providing a single test for overall group differences.\n",
        "**Example Scenario**\n",
        "- t-test: Comparing the mean scores of two classes (Class A vs. Class B) on a math test.\n",
        "- ANOVA: Comparing the mean scores of five classes (Class A, B, C, D, and E) on a math test."
      ],
      "metadata": {
        "id": "4-nW-B46BueA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.**\n",
        "\n"
      ],
      "metadata": {
        "id": "NlI2MqIIoEwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When to Use a One-Way ANOVA**\n",
        "\n",
        "A one-way ANOVA (Analysis of Variance) is used when you want to compare the means of three or more groups to determine if there is a statistically significant difference among them. The groups should be independent, and the data within each group should follow a normal distribution with similar variances.\n",
        "\n",
        "**Why Use One-Way ANOVA Instead of Multiple t-Tests?**\n",
        "\n",
        "1. Avoiding Type I Error Inflation:\n",
        "\n",
        "- Performing multiple t-tests increases the risk of a Type I error (falsely rejecting the null hypothesis).\n",
        "- For example:\n",
        " - If you compare three groups (A, B, and C), you need to run three t-tests (A vs. B, A vs. C, B vs. C).\n",
        " - With a 5% significance level for each test, the cumulative probability of making at least one Type I error increases significantly.\n",
        " - One-way ANOVA controls for this error by testing all groups simultaneously.\n",
        "\n",
        "2. Efficiency:\n",
        "\n",
        "- One-way ANOVA performs a single test to evaluate differences across all groups, saving time and effort compared to running multiple t-tests.\n",
        "3. Correct Interpretation:\n",
        "\n",
        "- One-way ANOVA tests whether there is any significant difference among the group means, while multiple t-tests focus only on pairwise comparisons.\n",
        "- ANOVA provides a broader picture of variability across groups.\n",
        "\n",
        "**Example Scenario:**\n",
        "\n",
        "- You have three groups of students taught by different methods (Group A, Group B, Group C), and you want to know if the teaching method significantly affects their test scores.\n",
        "- Instead of conducting t-tests (A vs. B, A vs. C, B vs. C), which could inflate Type I error, you would run a one-way ANOVA to test if the mean test scores differ across the groups.\n",
        "\n",
        "**Post-hoc Tests:**\n",
        "\n",
        "If the one-way ANOVA shows a significant result (p-value < 0.05), you can perform post-hoc tests (e.g., Tukey’s HSD) to identify which specific groups differ from each other."
      ],
      "metadata": {
        "id": "ypfbxxF70KDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?**"
      ],
      "metadata": {
        "id": "t8McnP7R4XoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partitioning Variance in ANOVA**\n",
        "\n",
        "In one-way ANOVA, the total variance in the data is divided into two components:\n",
        "\n",
        "1. Between-Group Variance (Explained Variance):\n",
        "\n",
        "- Measures the variation between the group means.\n",
        "- It represents how much the group means differ from the overall mean (grand mean).\n",
        "- A larger between-group variance suggests that the groups are significantly different from each other.\n",
        "\n",
        "2. Within-Group Variance (Unexplained Variance):\n",
        "\n",
        "- Measures the variation within each group.\n",
        "- It represents how much the individual data points vary around their group mean.\n",
        "- This reflects the natural variability in the data that cannot be attributed to differences between the groups.\n",
        "\n",
        "The total variance can thus be written as:\n",
        "\n",
        "Total Variance (SST)\n",
        "=\n",
        "Between-Group Variance (SSB)\n",
        "+\n",
        "Within-Group Variance (SSW)\n",
        "\n",
        "**Calculation of the F-Statistic**\n",
        "\n",
        "The F-statistic in ANOVA is a ratio that compares the explained variance (between-group) to the unexplained variance (within-group):\n",
        "\n",
        "F = $\\frac{\\text{Mean Square Between (MSB)}}{\\text{Mean Square Within (MSW)}}$\n",
        "\n",
        "Where:\n",
        "\n",
        "Sum of Squares Between (SSB): $SSB = \\sum_{i=1}^{k} n_i \\left( \\bar{Y}_i - \\bar{Y} \\right)^2$\n",
        "\n"
      ],
      "metadata": {
        "id": "zl5G6Qa84hdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Analysis of Variance (ANOVA), the total variance in the data is partitioned into two components: between-group variance and within-group variance. This partitioning is essential for determining whether there are significant differences between the means of different groups.\n",
        "\n",
        "**1 . Partitioning of Variance in ANOVA**\n",
        "\n",
        "The total variability in the dataset, known as Total Sum of Squares (SS_Total), is divided into:\n",
        "- Between-Group Variance (SS_Between): Variability due to differences between the group means.\n",
        "- Within-Group Variance (SS_Within): Variability due to differences within each group (i.e., individual variation around the group mean).\n",
        "\n",
        "Mathematically,  SS Total = SS Between + SS Within\n",
        "\n",
        "Where:\n",
        "\n",
        "SS_Total: Sum of squared differences between each observation and the overall mean.\n",
        "\n",
        "SS_Between: Sum of squared differences between each group mean and the overall mean.\n",
        "\n",
        "SS_Within: Sum of squared differences between each observation and its respective group mean.\n",
        "\n",
        "**2. Calculation of Mean Squares**\n",
        "\n",
        "After computing sum of squares, we obtain the Mean Squares (MS) by dividing each sum by its corresponding degrees of freedom (df):\n",
        "\n",
        "- **Between-Group Mean Square (MS_Between):**\n",
        "MS Between = $\\frac{\\text{SS Between}}{\\text{df Between}}$ = $\\frac{\\text{SS Between}}{\\text{k - 1}}$\n",
        "\n",
        "where k is the number of groups.\n",
        "\n",
        "- **Within-Group Mean Square (MS_Within):**\n",
        "MS Within = $\\frac{\\text{SS Within}}{\\text{df Within}}$ = $\\frac{\\text{SS Within}}{\\text{N - k}}$\n",
        "\n",
        "where N is the total number of observations.\n",
        "\n",
        "**3. Calculation of the F-Statistic**\n",
        "The F-statistic is used to test the null hypothesis that all group means are equal. It is computed as:\n",
        "F = $\\frac{\\text{MS Between}}{\\text{MS Within}}$\n",
        "\n",
        "**4. Interpretation of the F-Statistic**\n",
        "\n",
        "A large F-value suggests that between-group variance is much larger than within-group variance, indicating that at least one group mean significantly differs from the others.\n",
        "A small F-value suggests that the between-group variance is similar to the within-group variance, meaning the group differences could be due to random variation.\n",
        "Conclusion\n",
        "The partitioning of variance into between-group and within-group components allows us to quantify how much variability is due to actual differences between groups versus random noise within groups. This contributes to the calculation of the F-statistic, which helps in determining whether the observed differences among group means are statistically significant."
      ],
      "metadata": {
        "id": "ms1utg9K1BD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?**"
      ],
      "metadata": {
        "id": "PuWX7wZCzjkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison of Classical (Frequentist) ANOVA and Bayesian ANOVA  \n",
        "\n",
        "| **Aspect**              | **Frequentist (Classical) ANOVA** | **Bayesian ANOVA** |\n",
        "|---------------------|---------------------------------|---------------------|\n",
        "| **Approach**       | Based on null hypothesis significance testing (NHST). Uses sample data to infer whether observed differences are statistically significant. | Uses probability distributions to model uncertainty and update beliefs based on data. |\n",
        "| **Uncertainty Handling** | Assumes parameters (e.g., means) are fixed but unknown. Uncertainty is only due to sample variability. | Treats parameters as random variables with probability distributions, explicitly modeling uncertainty. |\n",
        "| **Parameter Estimation** | Estimates population means using sample means. Variability is assessed using variance components (MS_Between, MS_Within). | Uses prior distributions (e.g., normal for means, inverse-gamma for variance) and updates them using observed data (posterior distributions). |\n",
        "| **Hypothesis Testing** | Compares a null hypothesis (equal means) against an alternative using an **F-test**. A small p-value suggests rejecting \\( H_0 \\). | Compares different models using **Bayes Factors** (BF). Higher BF suggests stronger evidence for one model over another. |\n",
        "| **Interpretation of Results** | Relies on p-values: A result is \"significant\" if \\( p < 0.05 \\) (arbitrary threshold). | Provides **posterior probabilities**, allowing direct probability statements about hypotheses (e.g., probability that means are different). |\n",
        "| **Flexibility** | Assumes homogeneity of variances and normally distributed errors. Can be extended using mixed-effects models. | More flexible—can incorporate prior knowledge, hierarchical structures, and non-normal data. |\n",
        "| **Effect Size Estimation** | Uses measures like \\(\\eta^2\\) (eta squared) to quantify effect size. | Directly estimates effect sizes with credible intervals, avoiding reliance on p-values. |\n",
        "| **Computational Complexity** | Computationally simple and fast, requiring only sums of squares and F-ratios. | Computationally intensive, often requiring **MCMC (Markov Chain Monte Carlo)** sampling for posterior estimation. |\n",
        "\n",
        "## Key Takeaways  \n",
        "- **Frequentist ANOVA** focuses on hypothesis testing using p-values and assumes a fixed parameter framework.  \n",
        "- **Bayesian ANOVA** provides richer inference, explicitly modeling uncertainty and allowing probability-based conclusions.  \n",
        "- **Bayesian methods are advantageous** when dealing with small samples, prior information, hierarchical data, or model uncertainty.\n"
      ],
      "metadata": {
        "id": "eFUwiTMvzpDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Question: You have two sets of data representing the incomes of two different professions:**\n",
        "- **Profession A:** [48, 52, 55, 60, 62]\n",
        "\n",
        "- **Profession B:** [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions' incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "**Task:** Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "**Objective:** Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
      ],
      "metadata": {
        "id": "N4BC6NYU7mYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F-Test for Variance Comparison Between Two Professions' Incomes  \n",
        "\n",
        "## **Objective**  \n",
        "Perform an **F-test** to compare the variances of incomes between **Profession A** and **Profession B** using Python.  \n",
        "\n",
        "## **Data**  \n",
        "- **Profession A**: [48, 52, 55, 60, 62]  \n",
        "- **Profession B**: [45, 50, 55, 52, 47]  \n",
        "\n",
        "## **F-Test Formula**  \n",
        "The F-statistic is calculated as:  \n",
        "\n",
        "\\[\n",
        "F = \\frac{\\text{Variance of larger variance group}}{\\text{Variance of smaller variance group}}\n",
        "\\]\n",
        "\n",
        "The corresponding **p-value** helps determine if the variances are significantly different.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QtYs7drl8ZFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## **Python Implementation**\n",
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# Data\n",
        "profession_A = [48, 52, 55, 60, 62]\n",
        "profession_B = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Calculate sample variances\n",
        "var_A = np.var(profession_A, ddof=1)  # Unbiased variance (sample variance)\n",
        "var_B = np.var(profession_B, ddof=1)\n",
        "\n",
        "# Compute F-statistic\n",
        "F_stat = var_A / var_B if var_A > var_B else var_B / var_A\n",
        "\n",
        "# Degrees of freedom\n",
        "df1 = len(profession_A) - 1\n",
        "df2 = len(profession_B) - 1\n",
        "\n",
        "# Compute p-value (two-tailed test)\n",
        "p_value = 2 * min(f.cdf(F_stat, df1, df2), 1 - f.cdf(F_stat, df1, df2))\n",
        "\n",
        "print(f\"F-statistic: {F_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Conclusion: The variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Conclusion: No significant difference in variances.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNuPsdaC9BED",
        "outputId": "e24164b6-9f0d-44cf-9fd2-63d6dc1ca8ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.0892\n",
            "P-value: 0.4930\n",
            "Conclusion: No significant difference in variances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation of Results\n",
        "If p-value < 0.05 → Reject null hypothesis → Variances are significantly different.\n",
        "\n",
        "If p-value ≥ 0.05 → Fail to reject null hypothesis → Variances are not significantly different."
      ],
      "metadata": {
        "id": "9pNeXunI9Tug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data:**\n",
        "- **Region A:** [160, 162, 165, 158, 164]\n",
        "- **Region B:** [172, 175, 170, 168, 174]\n",
        "- **Region C:** [180, 182, 179, 185, 183]\n",
        "\n",
        "**Task:** Write Python code to perform the one-way ANOVA and interpret the results.\n",
        "\n",
        "**Objective:** Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
      ],
      "metadata": {
        "id": "yVaTBnh89m3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-Way ANOVA: Comparing Average Heights Across Three Regions  \n",
        "\n",
        "## **Objective**  \n",
        "Perform a **one-way ANOVA** to check if there is a significant difference in average heights between three regions.  \n",
        "\n",
        "## **Data**  \n",
        "- **Region A**: [160, 162, 165, 158, 164]  \n",
        "- **Region B**: [172, 175, 170, 168, 174]  \n",
        "- **Region C**: [180, 182, 179, 185, 183]  \n",
        "\n",
        "## **Hypotheses**  \n",
        "- **Null Hypothesis (\\(H_0\\))**: The average heights in all three regions are the same.  \n",
        "- **Alternative Hypothesis (\\(H_a\\))**: At least one region has a different average height.  \n",
        "\n"
      ],
      "metadata": {
        "id": "cBQewehD-eRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## **Python Code**\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Data\n",
        "region_A = [160, 162, 165, 158, 164]\n",
        "region_B = [172, 175, 170, 168, 174]\n",
        "region_C = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "F_stat, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "# Print results\n",
        "print(f\"F-statistic: {F_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Conclusion: There is a significant difference in average heights between regions.\")\n",
        "else:\n",
        "    print(\"Conclusion: No significant difference in average heights between regions.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3moG8S-9Btx",
        "outputId": "0fb55166-a0ee-44f1-9ef2-7e4a9d337a6d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.8733\n",
            "P-value: 0.0000\n",
            "Conclusion: There is a significant difference in average heights between regions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Interpretation of Results\n",
        "If p-value < 0.05 → Reject H 0 → At least one region's average height is different.\n",
        "\n",
        "If p-value ≥ 0.05 → Fail to reject H 0 → No significant difference in average heights."
      ],
      "metadata": {
        "id": "nLlG5gWY-ykd"
      }
    }
  ]
}